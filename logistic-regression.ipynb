{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0613b0e4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-30T15:58:08.720001Z",
     "iopub.status.busy": "2024-06-30T15:58:08.718967Z",
     "iopub.status.idle": "2024-06-30T15:58:09.533328Z",
     "shell.execute_reply": "2024-06-30T15:58:09.531941Z"
    },
    "papermill": {
     "duration": 0.824993,
     "end_time": "2024-06-30T15:58:09.536825",
     "exception": false,
     "start_time": "2024-06-30T15:58:08.711832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/datathon24playground/datathon-2024-playground/sample_submission.csv\n",
      "/kaggle/input/datathon24playground/datathon-2024-playground/ip_address_mapping.csv\n",
      "/kaggle/input/datathon24playground/datathon-2024-playground/train.csv\n",
      "/kaggle/input/datathon24playground/datathon-2024-playground/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a9273a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T15:58:09.548171Z",
     "iopub.status.busy": "2024-06-30T15:58:09.547388Z",
     "iopub.status.idle": "2024-06-30T15:58:09.556649Z",
     "shell.execute_reply": "2024-06-30T15:58:09.555517Z"
    },
    "papermill": {
     "duration": 0.017413,
     "end_time": "2024-06-30T15:58:09.558958",
     "exception": false,
     "start_time": "2024-06-30T15:58:09.541545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# # Load the training data\n",
    "# train = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/train.csv')\n",
    "# ip_mapping = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/ip_address_mapping.csv')\n",
    "\n",
    "# # Feature Engineering\n",
    "# train['alamat_IP'] = train['alamat_IP'].astype(float).astype(int)\n",
    "# train['waktu_pendaftaran_akun'] = pd.to_datetime(train['waktu_pendaftaran_akun'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# train['waktu_pembelian'] = pd.to_datetime(train['waktu_pembelian'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# train['selisih_hari'] = (train['waktu_pembelian'] - train['waktu_pendaftaran_akun']).dt.days\n",
    "\n",
    "# # Existing Feature Engineering\n",
    "# id_perangkat_freq = train['id_perangkat'].value_counts().to_dict()\n",
    "# train['freq_id_perangkat'] = train['id_perangkat'].map(id_perangkat_freq)\n",
    "\n",
    "# waktu_pembelian_freq = train['waktu_pembelian'].value_counts().to_dict()\n",
    "# train['freq_waktu_pembelian'] = train['waktu_pembelian'].map(waktu_pembelian_freq)\n",
    "\n",
    "# waktu_pendaftaran_akun_freq = train['waktu_pendaftaran_akun'].value_counts().to_dict()\n",
    "# train['freq_waktu_pendaftaran_akun'] = train['waktu_pendaftaran_akun'].map(waktu_pendaftaran_akun_freq)\n",
    "\n",
    "# train['freq_waktu_df_pb (mul)'] = train['freq_waktu_pendaftaran_akun'] * train['freq_waktu_pembelian']\n",
    "# train['freq_waktu_df_pb (add)'] = train['freq_waktu_pendaftaran_akun'] + train['freq_waktu_pembelian']\n",
    "\n",
    "# train['freq_waktu_pb_mul (div_mul)'] = train['freq_waktu_pembelian'] / train['freq_waktu_df_pb (mul)']\n",
    "\n",
    "# def find_country(ip):\n",
    "#     row = ip_mapping[(ip_mapping['batas_bawah_alamat_IP'] <= ip) & (ip_mapping['batas_atas_alamat_IP'] >= ip)]\n",
    "#     return row.iloc[0]['negara'] if not row.empty else 'Unknown'\n",
    "\n",
    "# train['negara'] = train['alamat_IP'].apply(find_country)\n",
    "\n",
    "# train['day_of_week_pendaftaran'] = train['waktu_pendaftaran_akun'].dt.dayofweek\n",
    "# train['hour_of_day_pendaftaran'] = train['waktu_pendaftaran_akun'].dt.hour\n",
    "# train['day_of_week_pembelian'] = train['waktu_pembelian'].dt.dayofweek\n",
    "# train['hour_of_day_pembelian'] = train['waktu_pembelian'].dt.hour\n",
    "\n",
    "# train['selisih_detik'] = (train['waktu_pembelian'] - train['waktu_pendaftaran_akun']).dt.total_seconds()\n",
    "# train['selisih_menit'] = train['selisih_detik'] / 60\n",
    "# train['selisih_jam'] = train['selisih_menit'] / 60\n",
    "\n",
    "# train['total_purchase_per_device'] = train.groupby('id_perangkat')['total_harga_pembelian'].transform('sum')\n",
    "# train['average_purchase_per_device'] = train.groupby('id_perangkat')['total_harga_pembelian'].transform('mean')\n",
    "\n",
    "# train['total_purchase_per_user'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('sum')\n",
    "# train['purchase_frequency_per_user'] = train.groupby('id_pengguna')['id_pengguna'].transform('count')\n",
    "# train['average_purchase_amount_per_user'] = train['total_purchase_per_user'] / train['purchase_frequency_per_user']\n",
    "\n",
    "# train['purchase_amount_per_day'] = train['total_harga_pembelian'] / train['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# age_bins = [0, 18, 25, 35, 45, 55, 65, np.inf]\n",
    "# age_labels = ['<18', '18-25', '26-35', '36-45', '46-55', '56-65', '65+']\n",
    "# train['age_group'] = pd.cut(train['umur'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# categorical_features = ['negara', 'sumber', 'browser', 'gender', 'age_group']\n",
    "# train = pd.get_dummies(train, columns=categorical_features)\n",
    "\n",
    "# train['purchase_amount_per_hour'] = train['total_harga_pembelian'] / train['selisih_jam'].replace(0, 1)\n",
    "# train['device_purchase_ratio'] = train['total_purchase_per_device'] / train['freq_id_perangkat'].replace(0, 1)\n",
    "# train['user_purchase_ratio'] = train['total_purchase_per_user'] / train['purchase_frequency_per_user'].replace(0, 1)\n",
    "\n",
    "# train['log_total_harga_pembelian'] = np.log1p(train['total_harga_pembelian'])\n",
    "# train['sqrt_selisih_hari'] = np.sqrt(train['selisih_hari'])\n",
    "\n",
    "# # Additional Feature Engineering\n",
    "# train['is_weekend_pendaftaran'] = train['day_of_week_pendaftaran'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "# train['is_weekend_pembelian'] = train['day_of_week_pembelian'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# train['total_amount_per_day'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('sum') / train['selisih_hari'].replace(0, 1)\n",
    "# train['variance_purchase_amount_per_user'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('var').fillna(0)\n",
    "# train['frequency_of_purchases'] = train.groupby('id_pengguna')['id_perangkat'].transform('count') / train['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# # Fill missing values with 0\n",
    "# train.fillna(0, inplace=True)\n",
    "\n",
    "# # Filter hanya kolom-kolom numerik setelah pengkodean kategori\n",
    "# numerical_features = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# correlations = train[numerical_features].corr()['fraud'].sort_values(ascending=False)\n",
    "\n",
    "# print(correlations)\n",
    "\n",
    "# # Pilih fitur-fitur dengan korelasi > 0.6 dengan 'fraud'\n",
    "# high_corr_features = correlations[correlations > 0.6].index.tolist()\n",
    "# high_corr_features.remove('fraud')  # Remove 'fraud' from the features list\n",
    "# print(f\"Features with correlation > 0.6: {high_corr_features}\")\n",
    "\n",
    "# # Prepare the dataset\n",
    "# X = train[high_corr_features]\n",
    "# y = train['fraud']\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Handle class imbalance with SMOTE\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_res = scaler.fit_transform(X_train_res)\n",
    "# X_val = scaler.transform(X_val)\n",
    "\n",
    "# # Train the Logistic Regression model\n",
    "# model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "# model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred = model.predict(X_val)\n",
    "# score = balanced_accuracy_score(y_val, y_pred)\n",
    "# print(f\"Balanced Accuracy Score: {score:.4f}\")\n",
    "# print(classification_report(y_val, y_pred))\n",
    "\n",
    "# # Visualization of the confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "\n",
    "# cm = confusion_matrix(y_val, y_pred)\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f79a9822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T15:58:09.570477Z",
     "iopub.status.busy": "2024-06-30T15:58:09.570108Z",
     "iopub.status.idle": "2024-06-30T15:58:09.577651Z",
     "shell.execute_reply": "2024-06-30T15:58:09.576553Z"
    },
    "papermill": {
     "duration": 0.016196,
     "end_time": "2024-06-30T15:58:09.579769",
     "exception": false,
     "start_time": "2024-06-30T15:58:09.563573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# high_corr_features = \n",
    "# # Load the test data\n",
    "# test = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/test.csv')\n",
    "\n",
    "# # Feature Engineering for test data (same as training data)\n",
    "# test['alamat_IP'] = test['alamat_IP'].astype(float).astype(int)\n",
    "# test['waktu_pendaftaran_akun'] = pd.to_datetime(test['waktu_pendaftaran_akun'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# test['waktu_pembelian'] = pd.to_datetime(test['waktu_pembelian'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# test['selisih_hari'] = (test['waktu_pembelian'] - test['waktu_pendaftaran_akun']).dt.days\n",
    "\n",
    "# id_perangkat_freq = test['id_perangkat'].value_counts().to_dict()\n",
    "# test['freq_id_perangkat'] = test['id_perangkat'].map(id_perangkat_freq)\n",
    "\n",
    "# waktu_pembelian_freq = test['waktu_pembelian'].value_counts().to_dict()\n",
    "# test['freq_waktu_pembelian'] = test['waktu_pembelian'].map(waktu_pembelian_freq)\n",
    "\n",
    "# waktu_pendaftaran_akun_freq = test['waktu_pendaftaran_akun'].value_counts().to_dict()\n",
    "# test['freq_waktu_pendaftaran_akun'] = test['waktu_pendaftaran_akun'].map(waktu_pendaftaran_akun_freq)\n",
    "\n",
    "# test['freq_waktu_df_pb (mul)'] = test['freq_waktu_pendaftaran_akun'] * test['freq_waktu_pembelian']\n",
    "# test['freq_waktu_df_pb (add)'] = test['freq_waktu_pendaftaran_akun'] + test['freq_waktu_pembelian']\n",
    "\n",
    "# test['freq_waktu_pb_mul (div_mul)'] = test['freq_waktu_pembelian'] / test['freq_waktu_df_pb (mul)']\n",
    "\n",
    "# def find_country(ip):\n",
    "#     row = ip_mapping[(ip_mapping['batas_bawah_alamat_IP'] <= ip) & (ip_mapping['batas_atas_alamat_IP'] >= ip)]\n",
    "#     return row.iloc[0]['negara'] if not row.empty else 'Unknown'\n",
    "\n",
    "# test['negara'] = test['alamat_IP'].apply(find_country)\n",
    "\n",
    "# test['day_of_week_pendaftaran'] = test['waktu_pendaftaran_akun'].dt.dayofweek\n",
    "# test['hour_of_day_pendaftaran'] = test['waktu_pendaftaran_akun'].dt.hour\n",
    "# test['day_of_week_pembelian'] = test['waktu_pembelian'].dt.dayofweek\n",
    "# test['hour_of_day_pembelian'] = test['waktu_pembelian'].dt.hour\n",
    "\n",
    "# test['selisih_detik'] = (test['waktu_pembelian'] - test['waktu_pendaftaran_akun']).dt.total_seconds()\n",
    "# test['selisih_menit'] = test['selisih_detik'] / 60\n",
    "# test['selisih_jam'] = test['selisih_menit'] / 60\n",
    "\n",
    "# test['total_purchase_per_device'] = test.groupby('id_perangkat')['total_harga_pembelian'].transform('sum')\n",
    "# test['average_purchase_per_device'] = test.groupby('id_perangkat')['total_harga_pembelian'].transform('mean')\n",
    "\n",
    "# test['total_purchase_per_user'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('sum')\n",
    "# test['purchase_frequency_per_user'] = test.groupby('id_pengguna')['id_pengguna'].transform('count')\n",
    "# test['average_purchase_amount_per_user'] = test['total_purchase_per_user'] / test['purchase_frequency_per_user']\n",
    "\n",
    "# test['purchase_amount_per_day'] = test['total_harga_pembelian'] / test['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# test['age_group'] = pd.cut(test['umur'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# test = pd.get_dummies(test, columns=categorical_features)\n",
    "\n",
    "# test['purchase_amount_per_hour'] = test['total_harga_pembelian'] / test['selisih_jam'].replace(0, 1)\n",
    "# test['device_purchase_ratio'] = test['total_purchase_per_device'] / test['freq_id_perangkat'].replace(0, 1)\n",
    "# test['user_purchase_ratio'] = test['total_purchase_per_user'] / test['purchase_frequency_per_user'].replace(0, 1)\n",
    "\n",
    "# test['log_total_harga_pembelian'] = np.log1p(test['total_harga_pembelian'])\n",
    "# test['sqrt_selisih_hari'] = np.sqrt(test['selisih_hari'])\n",
    "\n",
    "# # Additional Feature Engineering\n",
    "# test['is_weekend_pendaftaran'] = test['day_of_week_pendaftaran'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "# test['is_weekend_pembelian'] = test['day_of_week_pembelian'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# test['total_amount_per_day'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('sum') / test['selisih_hari'].replace(0, 1)\n",
    "# test['variance_purchase_amount_per_user'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('var').fillna(0)\n",
    "# test['frequency_of_purchases'] = test.groupby('id_pengguna')['id_perangkat'].transform('count') / test['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# # Fill missing values with 0\n",
    "# test.fillna(0, inplace=True)\n",
    "\n",
    "# # Filter hanya kolom-kolom numerik setelah pengkodean kategori\n",
    "# numerical_features = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# correlations = train[numerical_features].corr()['fraud'].sort_values(ascending=False)\n",
    "\n",
    "# print(correlations)\n",
    "\n",
    "# # Pilih fitur-fitur dengan korelasi > 0.6 dengan 'fraud'\n",
    "# high_corr_features = correlations[correlations > 0.6].index.tolist()\n",
    "# high_corr_features.remove('fraud')  # Remove 'fraud' from the features list\n",
    "# print(f\"Features with correlation > 0.6: {high_corr_features}\")\n",
    "\n",
    "# # Filter hanya kolom-kolom numerik\n",
    "# X_test = test[high_corr_features]\n",
    "\n",
    "# # Standardize features for test data using the scaler fitted on train data\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Predict using the trained model\n",
    "# y_pred_test = model.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission = pd.DataFrame({'id_pengguna': test['id_pengguna'], 'fraud': y_pred_test})\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e85eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T15:58:09.590919Z",
     "iopub.status.busy": "2024-06-30T15:58:09.590517Z",
     "iopub.status.idle": "2024-06-30T15:58:09.601888Z",
     "shell.execute_reply": "2024-06-30T15:58:09.600770Z"
    },
    "papermill": {
     "duration": 0.019774,
     "end_time": "2024-06-30T15:58:09.604100",
     "exception": false,
     "start_time": "2024-06-30T15:58:09.584326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# # Load the training data\n",
    "# train = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/train.csv')\n",
    "# ip_mapping = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/ip_address_mapping.csv')\n",
    "\n",
    "# # Feature Engineering\n",
    "# train['alamat_IP'] = train['alamat_IP'].astype(float).astype(int)\n",
    "# train['waktu_pendaftaran_akun'] = pd.to_datetime(train['waktu_pendaftaran_akun'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# train['waktu_pembelian'] = pd.to_datetime(train['waktu_pembelian'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# train['selisih_hari'] = (train['waktu_pembelian'] - train['waktu_pendaftaran_akun']).dt.days\n",
    "\n",
    "# # Existing Feature Engineering\n",
    "# id_perangkat_freq = train['id_perangkat'].value_counts().to_dict()\n",
    "# train['freq_id_perangkat'] = train['id_perangkat'].map(id_perangkat_freq)\n",
    "\n",
    "# waktu_pembelian_freq = train['waktu_pembelian'].value_counts().to_dict()\n",
    "# train['freq_waktu_pembelian'] = train['waktu_pembelian'].map(waktu_pembelian_freq)\n",
    "\n",
    "# waktu_pendaftaran_akun_freq = train['waktu_pendaftaran_akun'].value_counts().to_dict()\n",
    "# train['freq_waktu_pendaftaran_akun'] = train['waktu_pendaftaran_akun'].map(waktu_pendaftaran_akun_freq)\n",
    "\n",
    "# train['freq_waktu_df_pb (mul)'] = train['freq_waktu_pendaftaran_akun'] * train['freq_waktu_pembelian']\n",
    "# train['freq_waktu_df_pb (add)'] = train['freq_waktu_pendaftaran_akun'] + train['freq_waktu_pembelian']\n",
    "\n",
    "# train['freq_waktu_pb_mul (div_mul)'] = train['freq_waktu_pembelian'] / train['freq_waktu_df_pb (mul)']\n",
    "\n",
    "# def find_country(ip):\n",
    "#     row = ip_mapping[(ip_mapping['batas_bawah_alamat_IP'] <= ip) & (ip_mapping['batas_atas_alamat_IP'] >= ip)]\n",
    "#     return row.iloc[0]['negara'] if not row.empty else 'Unknown'\n",
    "\n",
    "# train['negara'] = train['alamat_IP'].apply(find_country)\n",
    "\n",
    "# train['day_of_week_pendaftaran'] = train['waktu_pendaftaran_akun'].dt.dayofweek\n",
    "# train['hour_of_day_pendaftaran'] = train['waktu_pendaftaran_akun'].dt.hour\n",
    "# train['day_of_week_pembelian'] = train['waktu_pembelian'].dt.dayofweek\n",
    "# train['hour_of_day_pembelian'] = train['waktu_pembelian'].dt.hour\n",
    "\n",
    "# train['selisih_detik'] = (train['waktu_pembelian'] - train['waktu_pendaftaran_akun']).dt.total_seconds()\n",
    "# train['selisih_menit'] = train['selisih_detik'] / 60\n",
    "# train['selisih_jam'] = train['selisih_menit'] / 60\n",
    "\n",
    "# train['total_purchase_per_device'] = train.groupby('id_perangkat')['total_harga_pembelian'].transform('sum')\n",
    "# train['average_purchase_per_device'] = train.groupby('id_perangkat')['total_harga_pembelian'].transform('mean')\n",
    "\n",
    "# train['total_purchase_per_user'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('sum')\n",
    "# train['purchase_frequency_per_user'] = train.groupby('id_pengguna')['id_pengguna'].transform('count')\n",
    "# train['average_purchase_amount_per_user'] = train['total_purchase_per_user'] / train['purchase_frequency_per_user']\n",
    "\n",
    "# train['purchase_amount_per_day'] = train['total_harga_pembelian'] / train['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# age_bins = [0, 18, 25, 35, 45, 55, 65, np.inf]\n",
    "# age_labels = ['<18', '18-25', '26-35', '36-45', '46-55', '56-65', '65+']\n",
    "# train['age_group'] = pd.cut(train['umur'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# categorical_features = ['negara', 'sumber', 'browser', 'gender', 'age_group']\n",
    "# train = pd.get_dummies(train, columns=categorical_features)\n",
    "\n",
    "# train['purchase_amount_per_hour'] = train['total_harga_pembelian'] / train['selisih_jam'].replace(0, 1)\n",
    "# train['device_purchase_ratio'] = train['total_purchase_per_device'] / train['freq_id_perangkat'].replace(0, 1)\n",
    "# train['user_purchase_ratio'] = train['total_purchase_per_user'] / train['purchase_frequency_per_user'].replace(0, 1)\n",
    "\n",
    "# train['log_total_harga_pembelian'] = np.log1p(train['total_harga_pembelian'])\n",
    "# train['sqrt_selisih_hari'] = np.sqrt(train['selisih_hari'])\n",
    "\n",
    "# # Additional Feature Engineering\n",
    "# train['is_weekend_pendaftaran'] = train['day_of_week_pendaftaran'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "# train['is_weekend_pembelian'] = train['day_of_week_pembelian'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# train['total_amount_per_day'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('sum') / train['selisih_hari'].replace(0, 1)\n",
    "# train['variance_purchase_amount_per_user'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('var').fillna(0)\n",
    "# train['frequency_of_purchases'] = train.groupby('id_pengguna')['id_perangkat'].transform('count') / train['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# train.fillna(0, inplace=True)\n",
    "\n",
    "# # Filter hanya kolom-kolom numerik setelah pengkodean kategori\n",
    "# numerical_features = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# correlations = train[numerical_features].corr()['fraud'].sort_values(ascending=False)\n",
    "\n",
    "# # Pilih fitur-fitur dengan korelasi > 0.6 dengan 'fraud'\n",
    "# high_corr_features = correlations[correlations > 0.6].index.tolist()\n",
    "# high_corr_features.remove('fraud')  # Remove 'fraud' from the features list\n",
    "\n",
    "# # Prepare the dataset\n",
    "# X_train = train[high_corr_features]\n",
    "# y_train = train['fraud']\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# # Train the Logistic Regression model\n",
    "# model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Load the test data\n",
    "# test = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/test.csv')\n",
    "\n",
    "# # Feature Engineering for test data (same as training data)\n",
    "# test['alamat_IP'] = test['alamat_IP'].astype(float).astype(int)\n",
    "# test['waktu_pendaftaran_akun'] = pd.to_datetime(test['waktu_pendaftaran_akun'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# test['waktu_pembelian'] = pd.to_datetime(test['waktu_pembelian'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# test['selisih_hari'] = (test['waktu_pembelian'] - test['waktu_pendaftaran_akun']).dt.days\n",
    "\n",
    "# id_perangkat_freq = test['id_perangkat'].value_counts().to_dict()\n",
    "# test['freq_id_perangkat'] = test['id_perangkat'].map(id_perangkat_freq)\n",
    "\n",
    "# waktu_pembelian_freq = test['waktu_pembelian'].value_counts().to_dict()\n",
    "# test['freq_waktu_pembelian'] = test['waktu_pembelian'].map(waktu_pembelian_freq)\n",
    "\n",
    "# waktu_pendaftaran_akun_freq = test['waktu_pendaftaran_akun'].value_counts().to_dict()\n",
    "# test['freq_waktu_pendaftaran_akun'] = test['waktu_pendaftaran_akun'].map(waktu_pendaftaran_akun_freq)\n",
    "\n",
    "# test['freq_waktu_df_pb (mul)'] = test['freq_waktu_pendaftaran_akun'] * test['freq_waktu_pembelian']\n",
    "# test['freq_waktu_df_pb (add)'] = test['freq_waktu_pendaftaran_akun'] + test['freq_waktu_pembelian']\n",
    "\n",
    "# test['freq_waktu_pb_mul (div_mul)'] = test['freq_waktu_pembelian'] / test['freq_waktu_df_pb (mul)']\n",
    "\n",
    "# def find_country(ip):\n",
    "#     row = ip_mapping[(ip_mapping['batas_bawah_alamat_IP'] <= ip) & (ip_mapping['batas_atas_alamat_IP'] >= ip)]\n",
    "#     return row.iloc[0]['negara'] if not row.empty else 'Unknown'\n",
    "\n",
    "# test['negara'] = test['alamat_IP'].apply(find_country)\n",
    "\n",
    "# test['day_of_week_pendaftaran'] = test['waktu_pendaftaran_akun'].dt.dayofweek\n",
    "# test['hour_of_day_pendaftaran'] = test['waktu_pendaftaran_akun'].dt.hour\n",
    "# test['day_of_week_pembelian'] = test['waktu_pembelian'].dt.dayofweek\n",
    "# test['hour_of_day_pembelian'] = test['waktu_pembelian'].dt.hour\n",
    "\n",
    "# test['selisih_detik'] = (test['waktu_pembelian'] - test['waktu_pendaftaran_akun']).dt.total_seconds()\n",
    "# test['selisih_menit'] = test['selisih_detik'] / 60\n",
    "# test['selisih_jam'] = test['selisih_menit'] / 60\n",
    "\n",
    "# test['total_purchase_per_device'] = test.groupby('id_perangkat')['total_harga_pembelian'].transform('sum')\n",
    "# test['average_purchase_per_device'] = test.groupby('id_perangkat')['total_harga_pembelian'].transform('mean')\n",
    "\n",
    "# test['total_purchase_per_user'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('sum')\n",
    "# test['purchase_frequency_per_user'] = test.groupby('id_pengguna')['id_pengguna'].transform('count')\n",
    "# test['average_purchase_amount_per_user'] = test['total_purchase_per_user'] / test['purchase_frequency_per_user']\n",
    "\n",
    "# test['purchase_amount_per_day'] = test['total_harga_pembelian'] / test['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# test['age_group'] = pd.cut(test['umur'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# test = pd.get_dummies(test, columns=categorical_features)\n",
    "\n",
    "# test['purchase_amount_per_hour'] = test['total_harga_pembelian'] / test['selisih_jam'].replace(0, 1)\n",
    "# test['device_purchase_ratio'] = test['total_purchase_per_device'] / test['freq_id_perangkat'].replace(0, 1)\n",
    "# test['user_purchase_ratio'] = test['total_purchase_per_user'] / test['purchase_frequency_per_user'].replace(0, 1)\n",
    "\n",
    "# test['log_total_harga_pembelian'] = np.log1p(test['total_harga_pembelian'])\n",
    "# test['sqrt_selisih_hari'] = np.sqrt(test['selisih_hari'])\n",
    "\n",
    "# # Additional Feature Engineering\n",
    "# test['is_weekend_pendaftaran'] = test['day_of_week_pendaftaran'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "# test['is_weekend_pembelian'] = test['day_of_week_pembelian'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# test['total_amount_per_day'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('sum') / test['selisih_hari'].replace(0, 1)\n",
    "# test['variance_purchase_amount_per_user'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('var').fillna(0)\n",
    "# test['frequency_of_purchases'] = test.groupby('id_pengguna')['id_perangkat'].transform('count') / test['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# # Fill missing values with 0\n",
    "# test.fillna(0, inplace=True)\n",
    "\n",
    "# # Filter hanya kolom-kolom numerik\n",
    "# X_test = test[high_corr_features]\n",
    "\n",
    "# # Standardize features for test data using the scaler fitted on train data\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Predict using the trained model\n",
    "# y_pred_test = model.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission = pd.DataFrame({'id_pengguna': test['id_pengguna'], 'fraud': y_pred_test})\n",
    "# submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb79419b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T15:58:09.614801Z",
     "iopub.status.busy": "2024-06-30T15:58:09.614446Z",
     "iopub.status.idle": "2024-06-30T15:58:09.620358Z",
     "shell.execute_reply": "2024-06-30T15:58:09.619317Z"
    },
    "papermill": {
     "duration": 0.013782,
     "end_time": "2024-06-30T15:58:09.622538",
     "exception": false,
     "start_time": "2024-06-30T15:58:09.608756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Impor library yang diperlukan\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# # Memuat dataset Iris\n",
    "# iris = load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# # Memisahkan data menjadi data pelatihan dan pengujian\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Penskalaan fitur menggunakan StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Inisialisasi model regresi logistik tanpa regularisasi (overfitting)\n",
    "# model_overfit = LogisticRegression(max_iter=1000, penalty='none', random_state=42)\n",
    "# model_overfit.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Inisialisasi model regresi logistik dengan regularisasi (L2 regularization)\n",
    "# model_regularized = LogisticRegression(max_iter=1000, penalty='l2', C=1.0, random_state=42)\n",
    "# model_regularized.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Prediksi menggunakan kedua model\n",
    "# y_pred_overfit = model_overfit.predict(X_test_scaled)\n",
    "# y_pred_regularized = model_regularized.predict(X_test_scaled)\n",
    "\n",
    "# # Evaluasi kinerja model\n",
    "# accuracy_overfit = accuracy_score(y_test, y_pred_overfit)\n",
    "# accuracy_regularized = accuracy_score(y_test, y_pred_regularized)\n",
    "\n",
    "# print(f\"Akurasi model overfitting: {accuracy_overfit:.2f}\")\n",
    "# print(f\"Akurasi model regularized: {accuracy_regularized:.2f}\")\n",
    "\n",
    "# # Plot confusion matrix untuk model regularized\n",
    "# cm = confusion_matrix(y_test, y_pred_regularized)\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.colorbar()\n",
    "# tick_marks = np.arange(len(iris.target_names))\n",
    "# plt.xticks(tick_marks, iris.target_names, rotation=45)\n",
    "# plt.yticks(tick_marks, iris.target_names)\n",
    "# plt.tight_layout()\n",
    "# plt.ylabel('True label')\n",
    "# plt.xlabel('Predicted label')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1143bfb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T15:58:09.633565Z",
     "iopub.status.busy": "2024-06-30T15:58:09.633232Z",
     "iopub.status.idle": "2024-06-30T15:58:09.644437Z",
     "shell.execute_reply": "2024-06-30T15:58:09.643385Z"
    },
    "papermill": {
     "duration": 0.019596,
     "end_time": "2024-06-30T15:58:09.646752",
     "exception": false,
     "start_time": "2024-06-30T15:58:09.627156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the training data\n",
    "# train = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/train.csv')\n",
    "# ip_mapping = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/ip_address_mapping.csv')\n",
    "\n",
    "# # Feature Engineering\n",
    "# train['alamat_IP'] = train['alamat_IP'].astype(float).astype(int)\n",
    "# train['waktu_pendaftaran_akun'] = pd.to_datetime(train['waktu_pendaftaran_akun'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# train['waktu_pembelian'] = pd.to_datetime(train['waktu_pembelian'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# train['selisih_hari'] = (train['waktu_pembelian'] - train['waktu_pendaftaran_akun']).dt.days\n",
    "\n",
    "# id_perangkat_freq = train['id_perangkat'].value_counts().to_dict()\n",
    "# train['freq_id_perangkat'] = train['id_perangkat'].map(id_perangkat_freq)\n",
    "\n",
    "# waktu_pembelian_freq = train['waktu_pembelian'].value_counts().to_dict()\n",
    "# train['freq_waktu_pembelian'] = train['waktu_pembelian'].map(waktu_pembelian_freq)\n",
    "\n",
    "# waktu_pendaftaran_akun_freq = train['waktu_pendaftaran_akun'].value_counts().to_dict()\n",
    "# train['freq_waktu_pendaftaran_akun'] = train['waktu_pendaftaran_akun'].map(waktu_pendaftaran_akun_freq)\n",
    "\n",
    "# train['freq_waktu_df_pb (mul)'] = train['freq_waktu_pendaftaran_akun'] * train['freq_waktu_pembelian']\n",
    "# train['freq_waktu_df_pb (add)'] = train['freq_waktu_pendaftaran_akun'] + train['freq_waktu_pembelian']\n",
    "# train['freq_waktu_pb_mul (div_mul)'] = train['freq_waktu_pembelian'] / train['freq_waktu_df_pb (mul)']\n",
    "\n",
    "# def find_country(ip):\n",
    "#     row = ip_mapping[(ip_mapping['batas_bawah_alamat_IP'] <= ip) & (ip_mapping['batas_atas_alamat_IP'] >= ip)]\n",
    "#     return row.iloc[0]['negara'] if not row.empty else 'Unknown'\n",
    "\n",
    "# train['negara'] = train['alamat_IP'].apply(find_country)\n",
    "\n",
    "# train['day_of_week_pendaftaran'] = train['waktu_pendaftaran_akun'].dt.dayofweek\n",
    "# train['hour_of_day_pendaftaran'] = train['waktu_pendaftaran_akun'].dt.hour\n",
    "# train['day_of_week_pembelian'] = train['waktu_pembelian'].dt.dayofweek\n",
    "# train['hour_of_day_pembelian'] = train['waktu_pembelian'].dt.hour\n",
    "\n",
    "# train['selisih_detik'] = (train['waktu_pembelian'] - train['waktu_pendaftaran_akun']).dt.total_seconds()\n",
    "# train['selisih_menit'] = train['selisih_detik'] / 60\n",
    "# train['selisih_jam'] = train['selisih_menit'] / 60\n",
    "\n",
    "# train['total_purchase_per_device'] = train.groupby('id_perangkat')['total_harga_pembelian'].transform('sum')\n",
    "# train['average_purchase_per_device'] = train.groupby('id_perangkat')['total_harga_pembelian'].transform('mean')\n",
    "\n",
    "# train['total_purchase_per_user'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('sum')\n",
    "# train['purchase_frequency_per_user'] = train.groupby('id_pengguna')['id_pengguna'].transform('count')\n",
    "# train['average_purchase_amount_per_user'] = train['total_purchase_per_user'] / train['purchase_frequency_per_user']\n",
    "\n",
    "# train['purchase_amount_per_day'] = train['total_harga_pembelian'] / train['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# age_bins = [0, 18, 25, 35, 45, 55, 65, np.inf]\n",
    "# age_labels = ['<18', '18-25', '26-35', '36-45', '46-55', '56-65', '65+']\n",
    "# train['age_group'] = pd.cut(train['umur'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# categorical_features = ['negara', 'sumber', 'browser', 'gender', 'age_group']\n",
    "# train = pd.get_dummies(train, columns=categorical_features)\n",
    "\n",
    "# train['purchase_amount_per_hour'] = train['total_harga_pembelian'] / train['selisih_jam'].replace(0, 1)\n",
    "# train['device_purchase_ratio'] = train['total_purchase_per_device'] / train['freq_id_perangkat'].replace(0, 1)\n",
    "# train['user_purchase_ratio'] = train['total_purchase_per_user'] / train['purchase_frequency_per_user'].replace(0, 1)\n",
    "\n",
    "# train['log_total_harga_pembelian'] = np.log1p(train['total_harga_pembelian'])\n",
    "# train['sqrt_selisih_hari'] = np.sqrt(train['selisih_hari'])\n",
    "\n",
    "# # Additional Feature Engineering\n",
    "# train['is_weekend_pendaftaran'] = train['day_of_week_pendaftaran'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "# train['is_weekend_pembelian'] = train['day_of_week_pembelian'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# train['total_amount_per_day'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('sum') / train['selisih_hari'].replace(0, 1)\n",
    "# train['variance_purchase_amount_per_user'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('var').fillna(0)\n",
    "# train['frequency_of_purchases'] = train.groupby('id_pengguna')['id_perangkat'].transform('count') / train['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# # Fill missing values with 0\n",
    "# train.fillna(0, inplace=True)\n",
    "\n",
    "# # Filter hanya kolom-kolom numerik setelah pengkodean kategori\n",
    "# numerical_features = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# correlations = train[numerical_features].corr()['fraud'].sort_values(ascending=False)\n",
    "\n",
    "# # Pilih fitur-fitur dengan korelasi > 0.6 dengan 'fraud'\n",
    "# high_corr_features = correlations[correlations > 0.6].index.tolist()\n",
    "# high_corr_features.remove('fraud')  # Remove 'fraud' from the features list\n",
    "\n",
    "# # Prepare the dataset\n",
    "# X = train[high_corr_features]\n",
    "# y = train['fraud']\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "\n",
    "# # Define the models\n",
    "# log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "# rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Create the VotingClassifier\n",
    "# voting_clf = VotingClassifier(estimators=[\n",
    "#     ('lr', log_reg),\n",
    "#     ('rf', rf_clf)\n",
    "# ], voting='soft')\n",
    "\n",
    "# # Train the VotingClassifier model\n",
    "# voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict and evaluate on validation set\n",
    "# y_pred = voting_clf.predict(X_val)\n",
    "# score = balanced_accuracy_score(y_val, y_pred)\n",
    "# print(f\"Balanced Accuracy Score: {score:.4f}\")\n",
    "# print(classification_report(y_val, y_pred))\n",
    "\n",
    "# # Visualization of the confusion matrix\n",
    "# cm = confusion_matrix(y_val, y_pred)\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.show()\n",
    "\n",
    "# # Load the test data\n",
    "# test = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/test.csv')\n",
    "\n",
    "# # Feature Engineering for test data (same as training data)\n",
    "# test['alamat_IP'] = test['alamat_IP'].astype(float).astype(int)\n",
    "# test['waktu_pendaftaran_akun'] = pd.to_datetime(test['waktu_pendaftaran_akun'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# test['waktu_pembelian'] = pd.to_datetime(test['waktu_pembelian'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# test['selisih_hari'] = (test['waktu_pembelian'] - test['waktu_pendaftaran_akun']).dt.days\n",
    "\n",
    "# id_perangkat_freq = test['id_perangkat'].value_counts().to_dict()\n",
    "# test['freq_id_perangkat'] = test['id_perangkat'].map(id_perangkat_freq)\n",
    "\n",
    "# waktu_pembelian_freq = test['waktu_pembelian'].value_counts().to_dict()\n",
    "# test['freq_waktu_pembelian'] = test['waktu_pembelian'].map(waktu_pembelian_freq)\n",
    "\n",
    "# waktu_pendaftaran_akun_freq = test['waktu_pendaftaran_akun'].value_counts().to_dict()\n",
    "# test['freq_waktu_pendaftaran_akun'] = test['waktu_pendaftaran_akun'].map(waktu_pendaftaran_akun_freq)\n",
    "\n",
    "# test['freq_waktu_df_pb (mul)'] = test['freq_waktu_pendaftaran_akun'] * test['freq_waktu_pembelian']\n",
    "# test['freq_waktu_df_pb (add)'] = test['freq_waktu_pendaftaran_akun'] + test['freq_waktu_pembelian']\n",
    "\n",
    "# test['freq_waktu_pb_mul (div_mul)'] = test['freq_waktu_pembelian'] / test['freq_waktu_df_pb (mul)']\n",
    "\n",
    "# def find_country(ip):\n",
    "#     row = ip_mapping[(ip_mapping['batas_bawah_alamat_IP'] <= ip) & (ip_mapping['batas_atas_alamat_IP'] >= ip)]\n",
    "#     return row.iloc[0]['negara'] if not row.empty else 'Unknown'\n",
    "\n",
    "# test['negara'] = test['alamat_IP'].apply(find_country)\n",
    "\n",
    "# test['day_of_week_pendaftaran'] = test['waktu_pendaftaran_akun'].dt.dayofweek\n",
    "# test['hour_of_day_pendaftaran'] = test['waktu_pendaftaran_akun'].dt.hour\n",
    "# test['day_of_week_pembelian'] = test['waktu_pembelian'].dt.dayofweek\n",
    "# test['hour_of_day_pembelian'] = test['waktu_pembelian'].dt.hour\n",
    "\n",
    "# test['selisih_detik'] = (test['waktu_pembelian'] - test['waktu_pendaftaran_akun']).dt.total_seconds()\n",
    "# test['selisih_menit'] = test['selisih_detik'] / 60\n",
    "# test['selisih_jam'] = test['selisih_menit'] / 60\n",
    "\n",
    "# test['total_purchase_per_device'] = test.groupby('id_perangkat')['total_harga_pembelian'].transform('sum')\n",
    "# test['average_purchase_per_device'] = test.groupby('id_perangkat')['total_harga_pembelian'].transform('mean')\n",
    "\n",
    "# test['total_purchase_per_user'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('sum')\n",
    "# test['purchase_frequency_per_user'] = test.groupby('id_pengguna')['id_pengguna'].transform('count')\n",
    "# test['average_purchase_amount_per_user'] = test['total_purchase_per_user'] / test['purchase_frequency_per_user']\n",
    "\n",
    "# test['purchase_amount_per_day'] = test['total_harga_pembelian'] / test['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# test['age_group'] = pd.cut(test['umur'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# test = pd.get_dummies(test, columns=categorical_features)\n",
    "\n",
    "# test['purchase_amount_per_hour'] = test['total_harga_pembelian'] / test['selisih_jam'].replace(0, 1)\n",
    "# test['device_purchase_ratio'] = test['total_purchase_per_device'] / test['freq_id_perangkat'].replace(0, 1)\n",
    "# test['user_purchase_ratio'] = test['total_purchase_per_user'] / test['purchase_frequency_per_user'].replace(0, 1)\n",
    "\n",
    "# test['log_total_harga_pembelian'] = np.log1p(test['total_harga_pembelian'])\n",
    "# test['sqrt_selisih_hari'] = np.sqrt(test['selisih_hari'])\n",
    "\n",
    "# # Additional Feature Engineering\n",
    "# test['is_weekend_pendaftaran'] = test['day_of_week_pendaftaran'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "# test['is_weekend_pembelian'] = test['day_of_week_pembelian'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# test['total_amount_per_day'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('sum') / test['selisih_hari'].replace(0, 1)\n",
    "# test['variance_purchase_amount_per_user'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('var').fillna(0)\n",
    "# test['frequency_of_purchases'] = test.groupby('id_pengguna')['id_perangkat'].transform('count') / test['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# # Fill missing values with 0\n",
    "# test.fillna(0, inplace=True)\n",
    "\n",
    "# # Filter hanya kolom-kolom numerik\n",
    "# X_test = test[high_corr_features]\n",
    "\n",
    "# # Standardize features for test data using the scaler fitted on train data\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Predict using the trained model\n",
    "# y_pred_test = voting_clf.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission = pd.DataFrame({'id_pengguna': test['id_pengguna'], 'fraud': y_pred_test})\n",
    "# submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca654a8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T15:58:09.658192Z",
     "iopub.status.busy": "2024-06-30T15:58:09.657822Z",
     "iopub.status.idle": "2024-06-30T15:58:09.668906Z",
     "shell.execute_reply": "2024-06-30T15:58:09.667905Z"
    },
    "papermill": {
     "duration": 0.019476,
     "end_time": "2024-06-30T15:58:09.670941",
     "exception": false,
     "start_time": "2024-06-30T15:58:09.651465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the training data\n",
    "# train = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/train.csv')\n",
    "# ip_mapping = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/ip_address_mapping.csv')\n",
    "\n",
    "# # Feature Engineering\n",
    "# train['alamat_IP'] = train['alamat_IP'].astype(float).astype(int)\n",
    "# train['waktu_pendaftaran_akun'] = pd.to_datetime(train['waktu_pendaftaran_akun'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# train['waktu_pembelian'] = pd.to_datetime(train['waktu_pembelian'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# train['selisih_hari'] = (train['waktu_pembelian'] - train['waktu_pendaftaran_akun']).dt.days\n",
    "\n",
    "# id_perangkat_freq = train['id_perangkat'].value_counts().to_dict()\n",
    "# train['freq_id_perangkat'] = train['id_perangkat'].map(id_perangkat_freq)\n",
    "\n",
    "# waktu_pembelian_freq = train['waktu_pembelian'].value_counts().to_dict()\n",
    "# train['freq_waktu_pembelian'] = train['waktu_pembelian'].map(waktu_pembelian_freq)\n",
    "\n",
    "# waktu_pendaftaran_akun_freq = train['waktu_pendaftaran_akun'].value_counts().to_dict()\n",
    "# train['freq_waktu_pendaftaran_akun'] = train['waktu_pendaftaran_akun'].map(waktu_pendaftaran_akun_freq)\n",
    "\n",
    "# train['freq_waktu_df_pb (mul)'] = train['freq_waktu_pendaftaran_akun'] * train['freq_waktu_pembelian']\n",
    "# train['freq_waktu_df_pb (add)'] = train['freq_waktu_pendaftaran_akun'] + train['freq_waktu_pembelian']\n",
    "# train['freq_waktu_pb_mul (div_mul)'] = train['freq_waktu_pembelian'] / train['freq_waktu_df_pb (mul)']\n",
    "\n",
    "# def find_country(ip):\n",
    "#     row = ip_mapping[(ip_mapping['batas_bawah_alamat_IP'] <= ip) & (ip_mapping['batas_atas_alamat_IP'] >= ip)]\n",
    "#     return row.iloc[0]['negara'] if not row.empty else 'Unknown'\n",
    "\n",
    "# train['negara'] = train['alamat_IP'].apply(find_country)\n",
    "\n",
    "# train['day_of_week_pendaftaran'] = train['waktu_pendaftaran_akun'].dt.dayofweek\n",
    "# train['hour_of_day_pendaftaran'] = train['waktu_pendaftaran_akun'].dt.hour\n",
    "# train['day_of_week_pembelian'] = train['waktu_pembelian'].dt.dayofweek\n",
    "# train['hour_of_day_pembelian'] = train['waktu_pembelian'].dt.hour\n",
    "\n",
    "# train['selisih_detik'] = (train['waktu_pembelian'] - train['waktu_pendaftaran_akun']).dt.total_seconds()\n",
    "# train['selisih_menit'] = train['selisih_detik'] / 60\n",
    "# train['selisih_jam'] = train['selisih_menit'] / 60\n",
    "\n",
    "# train['total_purchase_per_device'] = train.groupby('id_perangkat')['total_harga_pembelian'].transform('sum')\n",
    "# train['average_purchase_per_device'] = train.groupby('id_perangkat')['total_harga_pembelian'].transform('mean')\n",
    "\n",
    "# train['total_purchase_per_user'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('sum')\n",
    "# train['purchase_frequency_per_user'] = train.groupby('id_pengguna')['id_pengguna'].transform('count')\n",
    "# train['average_purchase_amount_per_user'] = train['total_purchase_per_user'] / train['purchase_frequency_per_user']\n",
    "\n",
    "# train['purchase_amount_per_day'] = train['total_harga_pembelian'] / train['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# age_bins = [0, 18, 25, 35, 45, 55, 65, np.inf]\n",
    "# age_labels = ['<18', '18-25', '26-35', '36-45', '46-55', '56-65', '65+']\n",
    "# train['age_group'] = pd.cut(train['umur'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# categorical_features = ['negara', 'sumber', 'browser', 'gender', 'age_group']\n",
    "# train = pd.get_dummies(train, columns=categorical_features)\n",
    "\n",
    "# train['purchase_amount_per_hour'] = train['total_harga_pembelian'] / train['selisih_jam'].replace(0, 1)\n",
    "# train['device_purchase_ratio'] = train['total_purchase_per_device'] / train['freq_id_perangkat'].replace(0, 1)\n",
    "# train['user_purchase_ratio'] = train['total_purchase_per_user'] / train['purchase_frequency_per_user'].replace(0, 1)\n",
    "\n",
    "# train['log_total_harga_pembelian'] = np.log1p(train['total_harga_pembelian'])\n",
    "# train['sqrt_selisih_hari'] = np.sqrt(train['selisih_hari'])\n",
    "\n",
    "# # Additional Feature Engineering\n",
    "# train['is_weekend_pendaftaran'] = train['day_of_week_pendaftaran'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "# train['is_weekend_pembelian'] = train['day_of_week_pembelian'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# train['total_amount_per_day'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('sum') / train['selisih_hari'].replace(0, 1)\n",
    "# train['variance_purchase_amount_per_user'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('var').fillna(0)\n",
    "# train['frequency_of_purchases'] = train.groupby('id_pengguna')['id_perangkat'].transform('count') / train['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# # Fill missing values with 0\n",
    "# train.fillna(0, inplace=True)\n",
    "\n",
    "# # Filter hanya kolom-kolom numerik setelah pengkodean kategori\n",
    "# numerical_features = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# correlations = train[numerical_features].corr()['fraud'].sort_values(ascending=False)\n",
    "\n",
    "# # Pilih fitur-fitur dengan korelasi > 0.6 dengan 'fraud'\n",
    "# high_corr_features = correlations[correlations > 0.6].index.tolist()\n",
    "# high_corr_features.remove('fraud')  # Remove 'fraud' from the features list\n",
    "\n",
    "# # Prepare the dataset\n",
    "# X = train[high_corr_features]\n",
    "# y = train['fraud']\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "\n",
    "# # Define the models\n",
    "# bnb = BernoulliNB()\n",
    "# lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# # Create the VotingClassifier with soft voting\n",
    "# voting_clf = VotingClassifier(estimators=[\n",
    "#     ('bnb', bnb),\n",
    "#     ('lda', lda)\n",
    "# ], voting='soft')\n",
    "\n",
    "# # Train the VotingClassifier model\n",
    "# voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict and evaluate on validation set\n",
    "# y_pred = voting_clf.predict(X_val)\n",
    "# score = balanced_accuracy_score(y_val, y_pred)\n",
    "# print(f\"Balanced Accuracy Score: {score:.4f}\")\n",
    "# print(classification_report(y_val, y_pred))\n",
    "\n",
    "# # Visualization of the confusion matrix\n",
    "# cm = confusion_matrix(y_val, y_pred)\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.show()\n",
    "\n",
    "# # Load the test data\n",
    "# test = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/test.csv')\n",
    "\n",
    "# # Feature Engineering for test data (same as training data)\n",
    "# test['alamat_IP'] = test['alamat_IP'].astype(float).astype(int)\n",
    "# test['waktu_pendaftaran_akun'] = pd.to_datetime(test['waktu_pendaftaran_akun'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# test['waktu_pembelian'] = pd.to_datetime(test['waktu_pembelian'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# test['selisih_hari'] = (test['waktu_pembelian'] - test['waktu_pendaftaran_akun']).dt.days\n",
    "\n",
    "# id_perangkat_freq = test['id_perangkat'].value_counts().to_dict()\n",
    "# test['freq_id_perangkat'] = test['id_perangkat'].map(id_perangkat_freq)\n",
    "\n",
    "# waktu_pembelian_freq = test['waktu_pembelian'].value_counts().to_dict()\n",
    "# test['freq_waktu_pembelian'] = test['waktu_pembelian'].map(waktu_pembelian_freq)\n",
    "\n",
    "# waktu_pendaftaran_akun_freq = test['waktu_pendaftaran_akun'].value_counts().to_dict()\n",
    "# test['freq_waktu_pendaftaran_akun'] = test['waktu_pendaftaran_akun'].map(waktu_pendaftaran_akun_freq)\n",
    "\n",
    "# test['freq_waktu_df_pb (mul)'] = test['freq_waktu_pendaftaran_akun'] * test['freq_waktu_pembelian']\n",
    "# test['freq_waktu_df_pb (add)'] = test['freq_waktu_pendaftaran_akun'] + test['freq_waktu_pembelian']\n",
    "# test['freq_waktu_pb_mul (div_mul)'] = test['freq_waktu_pembelian'] / test['freq_waktu_df_pb (mul)']\n",
    "\n",
    "# test['negara'] = test['alamat_IP'].apply(find_country)\n",
    "\n",
    "# test['day_of_week_pendaftaran'] = test['waktu_pendaftaran_akun'].dt.dayofweek\n",
    "# test['hour_of_day_pendaftaran'] = test['waktu_pendaftaran_akun'].dt.hour\n",
    "# test['day_of_week_pembelian'] = test['waktu_pembelian'].dt.dayofweek\n",
    "# test['hour_of_day_pembelian'] = test['waktu_pembelian'].dt.hour\n",
    "\n",
    "# test['selisih_detik'] = (test['waktu_pembelian'] - test['waktu_pendaftaran_akun']).dt.total_seconds()\n",
    "# test['selisih_menit'] = test['selisih_detik'] / 60\n",
    "# test['selisih_jam'] = test['selisih_menit'] / 60\n",
    "\n",
    "# test['total_purchase_per_device'] = test.groupby('id_perangkat')['total_harga_pembelian'].transform('sum')\n",
    "# test['average_purchase_per_device'] = test.groupby('id_perangkat')['total_harga_pembelian'].transform('mean')\n",
    "\n",
    "# test['total_purchase_per_user'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('sum')\n",
    "# test['purchase_frequency_per_user'] = test.groupby('id_pengguna')['id_pengguna'].transform('count')\n",
    "# test['average_purchase_amount_per_user'] = test['total_purchase_per_user'] / test['purchase_frequency_per_user']\n",
    "\n",
    "# test['purchase_amount_per_day'] = test['total_harga_pembelian'] / test['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# test['age_group'] = pd.cut(test['umur'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# test = pd.get_dummies(test, columns=categorical_features)\n",
    "\n",
    "# test['purchase_amount_per_hour'] = test['total_harga_pembelian'] / test['selisih_jam'].replace(0, 1)\n",
    "# test['device_purchase_ratio'] = test['total_purchase_per_device'] / test['freq_id_perangkat'].replace(0, 1)\n",
    "# test['user_purchase_ratio'] = test['total_purchase_per_user'] / test['purchase_frequency_per_user'].replace(0, 1)\n",
    "\n",
    "# test['log_total_harga_pembelian'] = np.log1p(test['total_harga_pembelian'])\n",
    "# test['sqrt_selisih_hari'] = np.sqrt(test['selisih_hari'])\n",
    "\n",
    "# test['is_weekend_pendaftaran'] = test['day_of_week_pendaftaran'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "# test['is_weekend_pembelian'] = test['day_of_week_pembelian'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# test['total_amount_per_day'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('sum') / test['selisih_hari'].replace(0, 1)\n",
    "# test['variance_purchase_amount_per_user'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('var').fillna(0)\n",
    "# test['frequency_of_purchases'] = test.groupby('id_pengguna')['id_perangkat'].transform('count') / test['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# test.fillna(0, inplace=True)\n",
    "\n",
    "# # Prepare test data\n",
    "# X_test = test[high_corr_features]\n",
    "\n",
    "# # Standardize test data\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Predict on test data\n",
    "# y_pred_test = voting_clf.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission = pd.DataFrame({'id_pembelian': test['id_pembelian'], 'fraud': y_pred_test})\n",
    "# submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a83c9e94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T15:58:09.683583Z",
     "iopub.status.busy": "2024-06-30T15:58:09.683228Z",
     "iopub.status.idle": "2024-06-30T15:58:09.694476Z",
     "shell.execute_reply": "2024-06-30T15:58:09.693526Z"
    },
    "papermill": {
     "duration": 0.019452,
     "end_time": "2024-06-30T15:58:09.696606",
     "exception": false,
     "start_time": "2024-06-30T15:58:09.677154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the training data\n",
    "# train = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/train.csv')\n",
    "# ip_mapping = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/ip_address_mapping.csv')\n",
    "\n",
    "# # Feature Engineering\n",
    "# train['alamat_IP'] = train['alamat_IP'].astype(float).astype(int)\n",
    "# train['waktu_pendaftaran_akun'] = pd.to_datetime(train['waktu_pendaftaran_akun'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# train['waktu_pembelian'] = pd.to_datetime(train['waktu_pembelian'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# train['selisih_hari'] = (train['waktu_pembelian'] - train['waktu_pendaftaran_akun']).dt.days\n",
    "\n",
    "# id_perangkat_freq = train['id_perangkat'].value_counts().to_dict()\n",
    "# train['freq_id_perangkat'] = train['id_perangkat'].map(id_perangkat_freq)\n",
    "\n",
    "# waktu_pembelian_freq = train['waktu_pembelian'].value_counts().to_dict()\n",
    "# train['freq_waktu_pembelian'] = train['waktu_pembelian'].map(waktu_pembelian_freq)\n",
    "\n",
    "# waktu_pendaftaran_akun_freq = train['waktu_pendaftaran_akun'].value_counts().to_dict()\n",
    "# train['freq_waktu_pendaftaran_akun'] = train['waktu_pendaftaran_akun'].map(waktu_pendaftaran_akun_freq)\n",
    "\n",
    "# train['freq_waktu_df_pb (mul)'] = train['freq_waktu_pendaftaran_akun'] * train['freq_waktu_pembelian']\n",
    "# train['freq_waktu_df_pb (add)'] = train['freq_waktu_pendaftaran_akun'] + train['freq_waktu_pembelian']\n",
    "# train['freq_waktu_pb_mul (div_mul)'] = train['freq_waktu_pembelian'] / train['freq_waktu_df_pb (mul)']\n",
    "\n",
    "# def find_country(ip):\n",
    "#     row = ip_mapping[(ip_mapping['batas_bawah_alamat_IP'] <= ip) & (ip_mapping['batas_atas_alamat_IP'] >= ip)]\n",
    "#     return row.iloc[0]['negara'] if not row.empty else 'Unknown'\n",
    "\n",
    "# train['negara'] = train['alamat_IP'].apply(find_country)\n",
    "\n",
    "# train['day_of_week_pendaftaran'] = train['waktu_pendaftaran_akun'].dt.dayofweek\n",
    "# train['hour_of_day_pendaftaran'] = train['waktu_pendaftaran_akun'].dt.hour\n",
    "# train['day_of_week_pembelian'] = train['waktu_pembelian'].dt.dayofweek\n",
    "# train['hour_of_day_pembelian'] = train['waktu_pembelian'].dt.hour\n",
    "\n",
    "# train['selisih_detik'] = (train['waktu_pembelian'] - train['waktu_pendaftaran_akun']).dt.total_seconds()\n",
    "# train['selisih_menit'] = train['selisih_detik'] / 60\n",
    "# train['selisih_jam'] = train['selisih_menit'] / 60\n",
    "\n",
    "# train['total_purchase_per_device'] = train.groupby('id_perangkat')['total_harga_pembelian'].transform('sum')\n",
    "# train['average_purchase_per_device'] = train.groupby('id_perangkat')['total_harga_pembelian'].transform('mean')\n",
    "\n",
    "# train['total_purchase_per_user'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('sum')\n",
    "# train['purchase_frequency_per_user'] = train.groupby('id_pengguna')['id_pengguna'].transform('count')\n",
    "# train['average_purchase_amount_per_user'] = train['total_purchase_per_user'] / train['purchase_frequency_per_user']\n",
    "\n",
    "# train['purchase_amount_per_day'] = train['total_harga_pembelian'] / train['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# age_bins = [0, 18, 25, 35, 45, 55, 65, np.inf]\n",
    "# age_labels = ['<18', '18-25', '26-35', '36-45', '46-55', '56-65', '65+']\n",
    "# train['age_group'] = pd.cut(train['umur'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# categorical_features = ['negara', 'sumber', 'browser', 'gender', 'age_group']\n",
    "# train = pd.get_dummies(train, columns=categorical_features)\n",
    "\n",
    "# train['purchase_amount_per_hour'] = train['total_harga_pembelian'] / train['selisih_jam'].replace(0, 1)\n",
    "# train['device_purchase_ratio'] = train['total_purchase_per_device'] / train['freq_id_perangkat'].replace(0, 1)\n",
    "# train['user_purchase_ratio'] = train['total_purchase_per_user'] / train['purchase_frequency_per_user'].replace(0, 1)\n",
    "\n",
    "# train['log_total_harga_pembelian'] = np.log1p(train['total_harga_pembelian'])\n",
    "# train['sqrt_selisih_hari'] = np.sqrt(train['selisih_hari'])\n",
    "\n",
    "# # Additional Feature Engineering\n",
    "# train['is_weekend_pendaftaran'] = train['day_of_week_pendaftaran'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "# train['is_weekend_pembelian'] = train['day_of_week_pembelian'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# train['total_amount_per_day'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('sum') / train['selisih_hari'].replace(0, 1)\n",
    "# train['variance_purchase_amount_per_user'] = train.groupby('id_pengguna')['total_harga_pembelian'].transform('var').fillna(0)\n",
    "# train['frequency_of_purchases'] = train.groupby('id_pengguna')['id_perangkat'].transform('count') / train['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# # Fill missing values with 0\n",
    "# train.fillna(0, inplace=True)\n",
    "\n",
    "# # Filter hanya kolom-kolom numerik setelah pengkodean kategori\n",
    "# numerical_features = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# correlations = train[numerical_features].corr()['fraud'].sort_values(ascending=False)\n",
    "\n",
    "# # Pilih fitur-fitur dengan korelasi > 0.6 dengan 'fraud'\n",
    "# high_corr_features = correlations[correlations > 0.6].index.tolist()\n",
    "# high_corr_features.remove('fraud')  # Remove 'fraud' from the features list\n",
    "\n",
    "# # Prepare the dataset\n",
    "# X = train[high_corr_features]\n",
    "# y = train['fraud']\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "\n",
    "# # Define the models\n",
    "# bnb = BernoulliNB()\n",
    "# lda = LinearDiscriminantAnalysis()\n",
    "# qda = QuadraticDiscriminantAnalysis()\n",
    "# log_reg = LogisticRegression()\n",
    "# knn = KNeighborsClassifier()\n",
    "\n",
    "# # Create the VotingClassifier with soft voting\n",
    "# voting_clf = VotingClassifier(estimators=[\n",
    "#     ('bnb', bnb),\n",
    "#     ('lda', lda),\n",
    "#     ('qda', qda),\n",
    "#     ('log_reg', log_reg),\n",
    "#     ('knn', knn)\n",
    "# ], voting='soft')\n",
    "\n",
    "# # Train the VotingClassifier model\n",
    "# voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict and evaluate on validation set\n",
    "# y_pred = voting_clf.predict(X_val)\n",
    "# score = balanced_accuracy_score(y_val, y_pred)\n",
    "# print(f\"Balanced Accuracy Score: {score:.4f}\")\n",
    "# print(classification_report(y_val, y_pred))\n",
    "\n",
    "# # Visualization of the confusion matrix\n",
    "# cm = confusion_matrix(y_val, y_pred)\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.show()\n",
    "\n",
    "# # Load the test data\n",
    "# test = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/test.csv')\n",
    "\n",
    "# # Feature Engineering for test data (same as training data)\n",
    "# test['alamat_IP'] = test['alamat_IP'].astype(float).astype(int)\n",
    "# test['waktu_pendaftaran_akun'] = pd.to_datetime(test['waktu_pendaftaran_akun'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# test['waktu_pembelian'] = pd.to_datetime(test['waktu_pembelian'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# test['selisih_hari'] = (test['waktu_pembelian'] - test['waktu_pendaftaran_akun']).dt.days\n",
    "\n",
    "# id_perangkat_freq = test['id_perangkat'].value_counts().to_dict()\n",
    "# test['freq_id_perangkat'] = test['id_perangkat'].map(id_perangkat_freq)\n",
    "\n",
    "# waktu_pembelian_freq = test['waktu_pembelian'].value_counts().to_dict()\n",
    "# test['freq_waktu_pembelian'] = test['waktu_pembelian'].map(waktu_pembelian_freq)\n",
    "\n",
    "# waktu_pendaftaran_akun_freq = test['waktu_pendaftaran_akun'].value_counts().to_dict()\n",
    "# test['freq_waktu_pendaftaran_akun'] = test['waktu_pendaftaran_akun'].map(waktu_pendaftaran_akun_freq)\n",
    "\n",
    "# test['freq_waktu_df_pb (mul)'] = test['freq_waktu_pendaftaran_akun'] * test['freq_waktu_pembelian']\n",
    "# test['freq_waktu_df_pb (add)'] = test['freq_waktu_pendaftaran_akun'] + test['freq_waktu_pembelian']\n",
    "# test['freq_waktu_pb_mul (div_mul)'] = test['freq_waktu_pembelian'] / test['freq_waktu_df_pb (mul)']\n",
    "\n",
    "# test['negara'] = test['alamat_IP'].apply(find_country)\n",
    "\n",
    "# test['day_of_week_pendaftaran'] = test['waktu_pendaftaran_akun'].dt.dayofweek\n",
    "# test['hour_of_day_pendaftaran'] = test['waktu_pendaftaran_akun'].dt.hour\n",
    "# test['day_of_week_pembelian'] = test['waktu_pembelian'].dt.dayofweek\n",
    "# test['hour_of_day_pembelian'] = test['waktu_pembelian'].dt.hour\n",
    "\n",
    "# test['selisih_detik'] = (test['waktu_pembelian'] - test['waktu_pendaftaran_akun']).dt.total_seconds()\n",
    "# test['selisih_menit'] = test['selisih_detik'] / 60\n",
    "# test['selisih_jam'] = test['selisih_menit'] / 60\n",
    "\n",
    "# test['total_purchase_per_device'] = test.groupby('id_perangkat')['total_harga_pembelian'].transform('sum')\n",
    "# test['average_purchase_per_device'] = test.groupby('id_perangkat')['total_harga_pembelian'].transform('mean')\n",
    "\n",
    "# test['total_purchase_per_user'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('sum')\n",
    "# test['purchase_frequency_per_user'] = test.groupby('id_pengguna')['id_pengguna'].transform('count')\n",
    "# test['average_purchase_amount_per_user'] = test['total_purchase_per_user'] / test['purchase_frequency_per_user']\n",
    "\n",
    "# test['purchase_amount_per_day'] = test['total_harga_pembelian'] / test['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# test['age_group'] = pd.cut(test['umur'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# test = pd.get_dummies(test, columns=categorical_features)\n",
    "\n",
    "# test['purchase_amount_per_hour'] = test['total_harga_pembelian'] / test['selisih_jam'].replace(0, 1)\n",
    "# test['device_purchase_ratio'] = test['total_purchase_per_device'] / test['freq_id_perangkat'].replace(0, 1)\n",
    "# test['user_purchase_ratio'] = test['total_purchase_per_user'] / test['purchase_frequency_per_user'].replace(0, 1)\n",
    "\n",
    "# test['log_total_harga_pembelian'] = np.log1p(test['total_harga_pembelian'])\n",
    "# test['sqrt_selisih_hari'] = np.sqrt(test['selisih_hari'])\n",
    "\n",
    "# test['is_weekend_pendaftaran'] = test['day_of_week_pendaftaran'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "# test['is_weekend_pembelian'] = test['day_of_week_pembelian'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# test['total_amount_per_day'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('sum') / test['selisih_hari'].replace(0, 1)\n",
    "# test['variance_purchase_amount_per_user'] = test.groupby('id_pengguna')['total_harga_pembelian'].transform('var').fillna(0)\n",
    "# test['frequency_of_purchases'] = test.groupby('id_pengguna')['id_perangkat'].transform('count') / test['selisih_hari'].replace(0, 1)\n",
    "\n",
    "# test.fillna(0, inplace=True)\n",
    "\n",
    "# # Prepare test data\n",
    "# X_test = test[high_corr_features]\n",
    "\n",
    "# # Standardize test data\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Predict on test data\n",
    "# y_pred_test = voting_clf.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission = pd.DataFrame({'id_pengguna': test['id_pengguna'], 'fraud': y_pred_test})\n",
    "# submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b135d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T15:58:09.707827Z",
     "iopub.status.busy": "2024-06-30T15:58:09.706908Z",
     "iopub.status.idle": "2024-06-30T16:30:31.384721Z",
     "shell.execute_reply": "2024-06-30T16:30:31.383477Z"
    },
    "papermill": {
     "duration": 1941.690347,
     "end_time": "2024-06-30T16:30:31.391590",
     "exception": false,
     "start_time": "2024-06-30T15:58:09.701243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/3754460348.py:16: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
      "  train_df = pd.merge(train_df, ip_mapping_df, left_on='alamat_IP', right_on='batas_bawah_alamat_IP', how='left')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy Score after ensembling: 0.7850\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the training data\n",
    "train_df = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/train.csv')\n",
    "ip_mapping_df = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/ip_address_mapping.csv')\n",
    "\n",
    "# Merge datasets based on IP address\n",
    "train_df = pd.merge(train_df, ip_mapping_df, left_on='alamat_IP', right_on='batas_bawah_alamat_IP', how='left')\n",
    "\n",
    "# Drop irrelevant columns\n",
    "train_df.drop(columns=['batas_bawah_alamat_IP', 'batas_atas_alamat_IP', 'alamat_IP'], inplace=True)\n",
    "\n",
    "# Feature engineering: Extracting date features\n",
    "train_df['waktu_pendaftaran_akun'] = pd.to_datetime(train_df['waktu_pendaftaran_akun'])\n",
    "train_df['waktu_pembelian'] = pd.to_datetime(train_df['waktu_pembelian'])\n",
    "\n",
    "train_df['registration_day'] = train_df['waktu_pendaftaran_akun'].dt.day\n",
    "train_df['registration_month'] = train_df['waktu_pendaftaran_akun'].dt.month\n",
    "train_df['registration_year'] = train_df['waktu_pendaftaran_akun'].dt.year\n",
    "train_df['purchase_day'] = train_df['waktu_pembelian'].dt.day\n",
    "train_df['purchase_month'] = train_df['waktu_pembelian'].dt.month\n",
    "train_df['purchase_year'] = train_df['waktu_pembelian'].dt.year\n",
    "\n",
    "train_df['time_difference'] = (train_df['waktu_pembelian'] - train_df['waktu_pendaftaran_akun']).dt.total_seconds()\n",
    "\n",
    "train_df.drop(columns=['waktu_pendaftaran_akun', 'waktu_pembelian'], inplace=True)\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = ['sumber', 'browser', 'gender', 'negara']\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    train_df[col] = le.fit_transform(train_df[col].astype(str))\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "X = train_df.drop(columns=['fraud', 'id_pengguna', 'id_perangkat'])\n",
    "y = train_df['fraud']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training with hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='balanced_accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Ensemble model\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "svc_clf = SVC(probability=True, random_state=42)\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('rf', best_rf), ('log_clf', log_clf), ('svc_clf', svc_clf), ('tree_clf', tree_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ensemble_clf.predict(X_val)\n",
    "balanced_acc = balanced_accuracy_score(y_val, y_pred)\n",
    "\n",
    "print(f'Balanced Accuracy Score after ensembling: {balanced_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "893d2206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T16:30:31.403620Z",
     "iopub.status.busy": "2024-06-30T16:30:31.402725Z",
     "iopub.status.idle": "2024-06-30T16:31:17.609037Z",
     "shell.execute_reply": "2024-06-30T16:31:17.608092Z"
    },
    "papermill": {
     "duration": 46.215236,
     "end_time": "2024-06-30T16:31:17.611588",
     "exception": false,
     "start_time": "2024-06-30T16:30:31.396352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/158609351.py:5: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
      "  test_df = pd.merge(test_df, ip_mapping_df, left_on='alamat_IP', right_on='batas_bawah_alamat_IP', how='left')\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "test_df = pd.read_csv('/kaggle/input/datathon24playground/datathon-2024-playground/test.csv')\n",
    "\n",
    "# Merge test dataset with IP address mapping\n",
    "test_df = pd.merge(test_df, ip_mapping_df, left_on='alamat_IP', right_on='batas_bawah_alamat_IP', how='left')\n",
    "\n",
    "# Drop irrelevant columns\n",
    "test_df.drop(columns=['batas_bawah_alamat_IP', 'batas_atas_alamat_IP', 'alamat_IP'], inplace=True)\n",
    "\n",
    "# Feature engineering: Extracting date features\n",
    "test_df['waktu_pendaftaran_akun'] = pd.to_datetime(test_df['waktu_pendaftaran_akun'])\n",
    "test_df['waktu_pembelian'] = pd.to_datetime(test_df['waktu_pembelian'])\n",
    "\n",
    "test_df['registration_day'] = test_df['waktu_pendaftaran_akun'].dt.day\n",
    "test_df['registration_month'] = test_df['waktu_pendaftaran_akun'].dt.month\n",
    "test_df['registration_year'] = test_df['waktu_pendaftaran_akun'].dt.year\n",
    "test_df['purchase_day'] = test_df['waktu_pembelian'].dt.day\n",
    "test_df['purchase_month'] = test_df['waktu_pembelian'].dt.month\n",
    "test_df['purchase_year'] = test_df['waktu_pembelian'].dt.year\n",
    "\n",
    "test_df['time_difference'] = (test_df['waktu_pembelian'] - test_df['waktu_pendaftaran_akun']).dt.total_seconds()\n",
    "\n",
    "test_df.drop(columns=['waktu_pendaftaran_akun', 'waktu_pembelian'], inplace=True)\n",
    "\n",
    "# Encode categorical features\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    test_df[col] = le.fit_transform(test_df[col].astype(str))\n",
    "\n",
    "# Preparing test data\n",
    "X_test = test_df.drop(columns=['id_pengguna', 'id_perangkat'])\n",
    "\n",
    "# Predict using the ensemble model\n",
    "y_test_pred = ensemble_clf.predict(X_test)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({'id_pengguna': test_df['id_pengguna'], 'fraud': y_test_pred})\n",
    "\n",
    "# Save submission to CSV\n",
    "submission.to_csv('submisi_hp.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8824564,
     "sourceId": 81458,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1994.273279,
   "end_time": "2024-06-30T16:31:20.240544",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-30T15:58:05.967265",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
